{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Spark Structured Streaming ve Delta Tables\n",
        "\n",
        "Spark, *Spark Structured Streaming* aracılığıyla veri akışı desteği sağlar ve bu desteği, akış verilerinin hedefleri (*havuzları*) veya *kaynakları* olabilecek *delta tabloları* aracılığıyla genişletir.\n",
        "\n",
        "Bu demo çalışmasında, cihazlardan gelen simüle edilmiş durum mesajlarından oluşan bir JSON dosyası klasöründen veri akışını almak için Spark'ı kullanacaksınız. Gerçek bir senaryoda veriler Kafka kuyruğu veya Azure Event Hub gibi başka bir gerçek zamanlı kaynaktan gelebilir.\n",
        "\n",
        "## Gelen veri akışı için bir klasör oluşturun\n",
        "\n",
        "1. Bu not defterinin Spark havuzunuza eklendiğinden emin olun (yukarıdaki **Attach to** açılır listesini kullanarak).\n",
        "2. Simüle edilen cihaz verilerinin yazılacağı **data** adlı bir klasör oluşturmak için aşağıdaki cell'i çalıştırın.\n",
        "\n",
        "    > **Not**: Spark havuzunun başlatılması gerektiğinden ilk cell'in çalışması biraz zaman alabilir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "\n",
        "# Create a folder\n",
        "inputPath = '/data/'\n",
        "mssparkutils.fs.mkdirs(inputPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bir veri streamingi sorgulamak için Spark Structured Streaming kullanın\n",
        "\n",
        "1. Cihazın adını ve durumunu içeren bir JSON şemasına göre klasördeki verileri okuyan bir stream veri framework oluşturmak için aşağıdaki cell'i çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a stream that reads data from the folder, using a JSON schema\n",
        "jsonSchema = StructType([\n",
        "  StructField(\"device\", StringType(), False),\n",
        "  StructField(\"status\", StringType(), False)\n",
        "])\n",
        "\n",
        "fileDF = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "2. Yukarıdaki cell'in tamamlanmasını bekleyin.\n",
        "3. Streaming dataframe oluşturulduğunda, verileri toplamak ve sonuçları bir çıkış streaminge yazmak için bir dönüştürme sorgusu uygulayabilirsiniz. Gelen stream cihaz verilerindeki hatalara göre filtrelemek ve cihaz başına hata sayısını saymak için aşağıdaki kodu çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "countDF = fileDF.filter(\"status == 'error'\").groupBy(\"device\").count()\n",
        "query = countDF.writeStream.format(\"memory\").queryName(\"counts\").outputMode(\"complete\").start()\n",
        "print('Streaming query started.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "4. Sorgu çıktısı bir bellek içi tabloya aktarılır. Bu tabloyu sorgulamak ve cihaz başına hata sayısını görüntülemek amacıyla SQL'i kullanmak için aşağıdaki cell'i çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "5. Henüz herhangi bir cihaz durum verisi yazmadığımız için sorgunun veri döndürmediğini unutmayın.\n",
        "6. Birkaç simüle edilmiş cihazdan bazı durum olayı verilerini yazarak bunu düzeltelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
        "\n",
        "mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "7. Toplu hata sayılarını görmek için SQL sorgusunu yeniden çalıştırın (sorgu hala veri döndürmüyorsa, birkaç saniye bekleyin ve tekrar deneyin!) Cihaz 1 için bir hata ve cihaz 2 için iki hata olmalıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "8. Hata sayısını not ederek sonuçları gözden geçirin. Daha sonra daha fazla cihaz verisi yazmak için aşağıdaki kodu çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
        "\n",
        "mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "9. Toplamalara yansıyan yeni durum olaylarını görmek için SQL sorgusunu yeniden çalıştırın (gerekirse birkaç saniye bekleyin). Artık cihaz 1 için iki hata ve cihaz 2 için üç hata olmalıdır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Delta tablosu oluşturma\n",
        "\n",
        "Azure Synapse Analytics, işlemlere, sürüm oluşturmaya ve diğer yararlı özelliklere destek eklemek için Spark Structured Streaming'i temel alan Linux Foundation *Delta Lake* mimarisini destekler.\n",
        "\n",
        "Özellikle, streaming verileri için hedef (veya *havuz*) olarak veya downstream  streaming sorguları için stream verilerinin *kaynağı* olarak *delta tabloları* oluşturabilirsiniz.\n",
        "\n",
        "Bunu keşfetmek için, daha önce oluşturduğumuz **data** klasörünü temel alan stream veri framework'ü, dosya sistemindeki bir konuma giden yolu kullanarak tanımlayacağımız yeni bir delta tablosuna yazacağız.\n",
        "\n",
        "1. Klasör verilerini bir delta tablosuna aktarmak için aşağıdaki cell'i çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "delta_table_path = inputPath + 'deltatable'\n",
        "stream = fileDF.writeStream.format(\"delta\").option(\"checkpointLocation\", inputPath + 'checkpoint').start(delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "2. Şimdi, kendisine aktarılan verileri görmek amacıyla delta tablosunu sorgulamak için sonraki cell'i çalıştırın. Sorgu ilk başta hiçbir veri döndürmezse, birkaç saniye bekleyin ve hücreyi yeniden çalıştırın)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Delta tabloları, zamanda önceki bir noktaya ait verileri görüntülemek için *time travel* adlı özelliği kullanmanızı sağlar.\n",
        "\n",
        "4. Dosya verilerinden stream'e alınan ilk mikro veri kümesini almak için aşağıdaki sorguyu çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "5. Artık Spark Structured Streaming ve delta tablolarını incelemeyi tamamladığınıza göre, veri akışını durdurun ve bu alıştırmada kullanılan dosyaları temizleyin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "stream.stop()\n",
        "query.stop()\n",
        "print(\"Stream stopped\")\n",
        "mssparkutils.fs.rm(inputPath, True)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
